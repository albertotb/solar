{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train and test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "\n",
    "df = (feather.read_dataframe('/home/SHARED/SOLAR/data/oahu_min.feather')\n",
    "             .set_index('Datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15722324/sliding-window-in-numpy\n",
    "def window_stack_forward(a, stepsize=1, width=3):\n",
    "    return np.hstack( a[i:1+i-width or None:stepsize] for i in range(0, width) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I feel this function can also be done for pd.DataFrame\n",
    "def window_stack(a, width=3):\n",
    "    n = a.shape[0]\n",
    "    return np.hstack(list(a[(width-1-i):(n-i)] for i in range(0, width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pandas 0.24, use df.to_numpy() instead of df.values. Also care with non-numeric columns\n",
    "width = 61\n",
    "a = window_stack(df.values, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "times   = [ ('t' if not idx else 't-{:d}'.format(idx)) for idx in range(width) ]\n",
    "columns = pd.MultiIndex.from_product((times, df.columns), names=('time', 'location'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrame, just for convenience of having indexes\n",
    "df_roll = pd.DataFrame(a, index=df.index[width-1:], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split target (time t) and variables (times t-1 to t-width+1)\n",
    "y = df_roll['t']\n",
    "X = df_roll.drop(columns='t', level='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train-test, approximately 12 and 4 months respectively\n",
    "X_train, X_test = X[:'2011-07-31'], X['2011-08-01':]\n",
    "y_train, y_test = y[:'2011-07-31'], y['2011-08-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(449885, 1140)\n",
      "(82892, 1140)\n",
      "(449885, 19)\n",
      "(82892, 19)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional predictor\n",
    "\n",
    "First we preprocess the dataset (for the moment, we'll just use as features the t-1 values at each sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use the previous timestep as features\n",
    "X_tr1 = X_train['t-1']\n",
    "y_tr1 = y_train\n",
    "\n",
    "X_te1 = X_test['t-1']\n",
    "y_te1 = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to use a 1D convolution, we are going to sort the sensors. For the initial test, we'll just sort them by longitude (from East to West). That way, nearer sensors are in close positions in the tensor, so the 1D convolution may extract useful correlations.\n",
    "\n",
    "Note: many other possible ordenations of the sensors could be added as new channels in the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# We load the info of the sensors to extract the longitude information\n",
    "info = pd.read_csv('/home/SHARED/SOLAR/data/info.csv')\n",
    "\n",
    "info.Location = info.Location.apply(lambda x: (x[:2] + x[-2:]).replace('_', ''))\n",
    "info.index = info.Location\n",
    "# Sorted longitudes\n",
    "longs = info['       Longitude'].sort_values(ascending=False)\n",
    "\n",
    "# We drop two sensors (they are different compared to the other 17, since they are \"tilted\")\n",
    "X_tr1.drop('GT_AP6', inplace=True, axis=1)\n",
    "y_tr1.drop('GT_AP6', inplace=True, axis=1)\n",
    "X_tr1.drop('GT_DH1', inplace=True, axis=1)\n",
    "y_tr1.drop('GT_DH1', inplace=True, axis=1)\n",
    "X_te1.drop('GT_AP6', inplace=True, axis=1)\n",
    "y_te1.drop('GT_AP6', inplace=True, axis=1)\n",
    "X_te1.drop('GT_DH1', inplace=True, axis=1)\n",
    "y_te1.drop('GT_DH1', inplace=True, axis=1)\n",
    "\n",
    "# Just some auxiliar code to homogeneize name of sensors across different tables\n",
    "homogen_name = lambda x: x[-4:].replace('_', '')\n",
    "X_tr1.columns = [homogen_name(x) for x in X_tr1.columns.values.tolist()]\n",
    "y_tr1.columns = [homogen_name(x) for x in y_tr1.columns.values.tolist()]\n",
    "X_te1.columns = [homogen_name(x) for x in X_te1.columns.values.tolist()]\n",
    "y_te1.columns = [homogen_name(x) for x in y_te1.columns.values.tolist()]\n",
    "\n",
    "\n",
    "# Finally, we sort the data according to sensor's longitude\n",
    "X_tr1_1 = X_tr1[longs.index]\n",
    "y_tr1_1 = y_tr1[longs.index]\n",
    "X_te1_1 = X_te1[longs.index]\n",
    "y_te1_1 = y_te1[longs.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify which sensor do we want to predict and test.\n",
    "\n",
    "(In the future, we need to discuss how are we going to predict, if just by looping over each sensor, or just give a vectorial prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape, Add, Multiply, Subtract, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, LocallyConnected1D, Conv1D, UpSampling1D, MaxPooling1D, Dot, Concatenate\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture is defined below.\n",
    "\n",
    "Some highlights:\n",
    "* Locally connected works better than pure convolutional at the first layers (probably because the sensors at not located in a uniform grid)\n",
    "* Trick to improve acc: add a final layer combining the convolutional prediction with the persistance prediction, so in case the input is \"strange\", the model could learn to output the persistance prediction (i.e., the previous time-step), which is somewhat reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_sensor(index_sensor, n_sensors=17):\n",
    "    ''' Returns a model using all the sensors to predict index_sensor '''\n",
    "    xin = Input(shape=(n_sensors,1), name='main_input')\n",
    "    x = LocallyConnected1D(8, 7, data_format = 'channels_last', padding='valid')(xin)\n",
    "    x = Activation('relu')(x)\n",
    "    x = LocallyConnected1D(16, 5, data_format = 'channels_last', padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, data_format = 'channels_last', padding='causal')(x)\n",
    "    xl = Flatten()(x)\n",
    "    xl = Dropout(0.2)(xl)\n",
    "    xo = Dense(1)(xl)\n",
    "\n",
    "    # use date info here?\n",
    "    xinf = Flatten()(xin)\n",
    "    s  = Dense(5)(xinf)\n",
    "    s = Activation('tanh')(s)\n",
    "    s = Dense(2)(s)\n",
    "    s = Activation('softmax')(s)\n",
    "\n",
    "\n",
    "    xin_0 = Activation('relu')(xin)\n",
    "    xin_1 = Lambda(lambda x : x[:,index_sensor,:])(xin_0)\n",
    "    xo_m = Dot(axes=1)([Concatenate()([xo,xin_1]), s])\n",
    "    # instead of this, use a residual connection\n",
    "\n",
    "    #xom = Add()([xo, xin_1])\n",
    "\n",
    "    # better than just residual (xom)\n",
    "    xo_m = Activation('relu')(xo_m)\n",
    "\n",
    "    model = Model(inputs=[xin], outputs=[xo_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "lr = 0.0001\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "# We add a callback to log metrics and another one to schedule the learning rate\n",
    "\n",
    "#see clr.py in this same folder\n",
    "from clr import CyclicLR\n",
    "\n",
    "c1 = keras.callbacks.BaseLogger(stateful_metrics=None)\n",
    "c2 = CyclicLR(step_size=250, base_lr=lr)\n",
    "c3 = keras.callbacks.History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. The below configuration should take 2 minutes in a 16 core CPU\n",
    "(no GPU needed). We are using a huge batch-size to speed up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sensors = 17\n",
    "sensor = 'AP5'  # 'AP5' initial\n",
    "index_sensor = 4\n",
    "\n",
    "def make_features(sensor='AP5'):\n",
    "    X_tr1_1_np = X_tr1_1.values\n",
    "    y_tr1_1_np = y_tr1_1[sensor].values\n",
    "\n",
    "    X_te1_1_np = X_te1_1.values\n",
    "    y_te1_1_np = y_te1_1[sensor].values\n",
    "    return X_tr1_1_np, y_tr1_1_np, X_te1_1_np, y_te1_1_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1 << 11   # as big as possible so we can explore many models\n",
    "epochs = 1 << 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final MAE for AP5 sensor is around 40.2\n",
    "# Other sensors are also around 40-50.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_sensor(id_sensor=4):\n",
    "    longs_np = longs.index.values\n",
    "    X_tr1_1_np, y_tr1_1_np, X_te1_1_np, y_te1_1_np = make_features(sensor=longs_np[id_sensor])\n",
    "    model = make_model_sensor(id_sensor, n_sensors=17)\n",
    "    model.compile(opt, loss='mean_absolute_error')\n",
    "    model.fit(np.atleast_3d(X_tr1_1_np), y_tr1_1_np, batch_size=batch_size, epochs=epochs, validation_data=\n",
    "          (np.atleast_3d(X_te1_1_np),y_te1_1_np), callbacks=[c1, c2, c3])\n",
    "    return longs_np[id_sensor], c3.history['val_loss'][-1]\n",
    "    # Note: for this quick trial, we are using as validation the test!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AP7\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      " 14336/449885 [..............................] - ETA: 54s - loss: 82.6605 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.300463). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449885/449885 [==============================] - 8s 18us/step - loss: 62.0225 - val_loss: 57.2572\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 59.2373 - val_loss: 56.9693\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 59.1477 - val_loss: 56.9432\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 59.1216 - val_loss: 56.9722\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 59.0265 - val_loss: 56.8951\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 59.0398 - val_loss: 56.7696\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.9424 - val_loss: 56.7727\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.9254 - val_loss: 57.0870\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.9006 - val_loss: 56.7262\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.8344 - val_loss: 56.6765\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.8564 - val_loss: 56.5941\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 58.7335 - val_loss: 56.6338\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 58.8333 - val_loss: 56.5956\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.6817 - val_loss: 56.6533\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 58.7530 - val_loss: 56.6625\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 58.6473 - val_loss: 56.6126\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.6541 - val_loss: 56.5197\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.6666 - val_loss: 56.5979\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.5407 - val_loss: 56.6627\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.6330 - val_loss: 56.7352\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 58.4742 - val_loss: 56.6517\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 58.5709 - val_loss: 56.7399\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 58.4365 - val_loss: 56.6453\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.5234 - val_loss: 56.6761\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.4359 - val_loss: 56.6039\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 58.4029 - val_loss: 56.6387\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 58.4411 - val_loss: 56.5908\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 58.2970 - val_loss: 56.5278\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.4875 - val_loss: 56.5463\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 58.2725 - val_loss: 56.7049\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.4239 - val_loss: 56.5840\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 58.3379 - val_loss: 56.5087\n",
      "1 AP6\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 7s 17us/step - loss: 62.7666 - val_loss: 58.0463\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.0140 - val_loss: 57.6977\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 57.6618 - val_loss: 57.8220\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 57.6038 - val_loss: 57.6874\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 57.4144 - val_loss: 57.8526\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 57.5170 - val_loss: 57.6820\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 57.3265 - val_loss: 57.8283\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 57.3842 - val_loss: 58.1368\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 57.2898 - val_loss: 57.9004\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 57.2957 - val_loss: 57.9838\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 57.3418 - val_loss: 58.1580\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 57.2179 - val_loss: 58.2091\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 57.1358 - val_loss: 58.0305\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 56.9478 - val_loss: 58.0967\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 57.0568 - val_loss: 58.6623\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 56.8864 - val_loss: 58.6041\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 56.9120 - val_loss: 58.6368\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 56.7650 - val_loss: 58.3846\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 56.7130 - val_loss: 58.3463\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 56.7173 - val_loss: 58.7277\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 56.5635 - val_loss: 58.4886\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 56.6240 - val_loss: 58.4652\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 7s 15us/step - loss: 56.4561 - val_loss: 58.6139\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 56.5873 - val_loss: 58.8989\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 56.4068 - val_loss: 58.6690\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 56.4565 - val_loss: 59.5952\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 56.3821 - val_loss: 58.3412\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 56.3139 - val_loss: 58.5872\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 56.3833 - val_loss: 58.4711\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 56.1936 - val_loss: 58.5103\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 56.3119 - val_loss: 58.0322\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 56.1544 - val_loss: 57.9215\n",
      "2 AP4\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 8s 17us/step - loss: 57.6170 - val_loss: 55.0147\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 53.8456 - val_loss: 53.7315\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 53.3577 - val_loss: 53.6991\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 52.7347 - val_loss: 53.1007\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 52.4989 - val_loss: 53.1199\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 52.5652 - val_loss: 53.3706\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 52.3660 - val_loss: 53.1141\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 52.5010 - val_loss: 52.8806\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 52.2452 - val_loss: 52.9752\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 52.3564 - val_loss: 53.1146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 52.2173 - val_loss: 52.7729\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 52.1129 - val_loss: 52.8408\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 52.1177 - val_loss: 52.5343\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 51.9374 - val_loss: 52.6950\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 51.9738 - val_loss: 52.5467\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 51.7626 - val_loss: 52.1049\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 51.9115 - val_loss: 52.0424\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 51.6568 - val_loss: 52.0198\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 51.6815 - val_loss: 52.1560\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 51.6534 - val_loss: 51.8213\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 51.5544 - val_loss: 51.7318\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 51.6862 - val_loss: 51.8606\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 51.4916 - val_loss: 51.7920\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 51.6619 - val_loss: 52.2727\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 51.4908 - val_loss: 51.6964\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 51.5995 - val_loss: 52.0181\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 51.5036 - val_loss: 51.7119\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 51.5003 - val_loss: 51.8532\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 51.5151 - val_loss: 51.6338\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 51.3550 - val_loss: 51.8903\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 51.4570 - val_loss: 51.5802\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 51.2861 - val_loss: 51.3379\n",
      "3 AP3\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 8s 17us/step - loss: 46.5672 - val_loss: 1.2993\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 43.7885 - val_loss: 10.6761\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 42.5967 - val_loss: 11.6912\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 41.3130 - val_loss: 10.6059\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 40.9452 - val_loss: 10.5760\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 40.6612 - val_loss: 11.6773\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 40.2745 - val_loss: 11.0212\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 40.4275 - val_loss: 12.1103\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 39.9305 - val_loss: 11.4726\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 40.0478 - val_loss: 8.6572\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 39.6930 - val_loss: 9.2849\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 39.7548 - val_loss: 10.7985\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 39.6255 - val_loss: 9.1627\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 39.4840 - val_loss: 7.8517\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 39.6120 - val_loss: 9.2287\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 39.2974 - val_loss: 9.8454\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 39.5177 - val_loss: 9.5278\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 39.1312 - val_loss: 9.7117\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 39.4337 - val_loss: 10.7097\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 39.0928 - val_loss: 9.8226\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 39.0839 - val_loss: 9.6876\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 39.0704 - val_loss: 9.6668\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 38.9025 - val_loss: 9.8812\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 39.0874 - val_loss: 9.5334\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 38.7896 - val_loss: 9.0705\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 39.0781 - val_loss: 9.7585\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 38.7941 - val_loss: 9.8104\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 38.9883 - val_loss: 8.8927\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 38.8670 - val_loss: 8.8263\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 38.7731 - val_loss: 8.6430\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 38.9254 - val_loss: 9.4978\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 38.7118 - val_loss: 9.6895\n",
      "4 AP5\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 8s 18us/step - loss: 54.8077 - val_loss: 44.8206\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 46.4932 - val_loss: 43.1272\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.8769 - val_loss: 42.5883\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.9566 - val_loss: 41.6450\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 44.9526 - val_loss: 41.7110\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.7043 - val_loss: 41.5110\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.6036 - val_loss: 41.8692\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 44.6111 - val_loss: 41.3112\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.4110 - val_loss: 41.2323\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.5305 - val_loss: 41.2363\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.3454 - val_loss: 41.1652\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.4205 - val_loss: 41.3283\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 44.3099 - val_loss: 41.1405\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.3446 - val_loss: 41.2533\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.3519 - val_loss: 41.0406\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.1633 - val_loss: 41.0023\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.3159 - val_loss: 41.0955\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.0933 - val_loss: 41.3492\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.2597 - val_loss: 41.0442\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.0899 - val_loss: 40.9081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 44.2361 - val_loss: 40.9814\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 44.0844 - val_loss: 40.8825\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.1257 - val_loss: 41.3094\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 44.1115 - val_loss: 40.9257\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.0013 - val_loss: 40.9864\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.2495 - val_loss: 40.8569\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 43.9804 - val_loss: 40.9677\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.1350 - val_loss: 41.1542\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 43.9751 - val_loss: 40.7533\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.0282 - val_loss: 40.9065\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 43.9821 - val_loss: 40.7261\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 43.9451 - val_loss: 41.7387\n",
      "5 AP1\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 8s 18us/step - loss: 59.4543 - val_loss: 57.7048\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 58.0711 - val_loss: 57.6854\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 56.6476 - val_loss: 52.8719\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.1315 - val_loss: 50.3302\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 48.2569 - val_loss: 49.7435\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 46.6103 - val_loss: 49.0667\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 46.3915 - val_loss: 49.1609\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 46.4519 - val_loss: 49.1218\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.9211 - val_loss: 49.3627\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 45.9130 - val_loss: 50.1390\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.4298 - val_loss: 49.5636\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.6637 - val_loss: 49.1836\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 45.3077 - val_loss: 49.6525\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 7s 15us/step - loss: 45.4773 - val_loss: 50.0233\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 45.2086 - val_loss: 49.3132\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.2153 - val_loss: 49.0689\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.3538 - val_loss: 49.0908\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.0360 - val_loss: 48.6269\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.1972 - val_loss: 48.7959\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.8756 - val_loss: 48.6389\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.0412 - val_loss: 48.3540\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.6868 - val_loss: 48.4095\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.6538 - val_loss: 48.2213\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 44.4595 - val_loss: 47.6609\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.2312 - val_loss: 48.3641\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.2971 - val_loss: 47.7030\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 43.9851 - val_loss: 47.6317\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.1297 - val_loss: 47.9517\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 43.8310 - val_loss: 47.2404\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 43.9770 - val_loss: 47.4084\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.0108 - val_loss: 47.2330\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 43.8279 - val_loss: 47.5362\n",
      "6 DH5\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 7s 16us/step - loss: 59.9189 - val_loss: 54.6690\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 53.1736 - val_loss: 52.8155\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 50.3121 - val_loss: 52.3524\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 49.6313 - val_loss: 52.0908\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.5127 - val_loss: 51.3459\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.2354 - val_loss: 51.3287\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 49.3043 - val_loss: 52.4426\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 49.1834 - val_loss: 51.3386\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.1541 - val_loss: 51.4974\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.2029 - val_loss: 51.3315\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.0278 - val_loss: 51.5521\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 49.1864 - val_loss: 51.1267\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 48.9782 - val_loss: 51.3170\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.1582 - val_loss: 51.2387\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 49.0497 - val_loss: 51.1939\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 49.0526 - val_loss: 51.2943\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.0595 - val_loss: 51.1423\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 48.9663 - val_loss: 50.9780\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 49.0441 - val_loss: 51.4526\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 48.9314 - val_loss: 51.2821\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 49.1142 - val_loss: 52.3139\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 48.9627 - val_loss: 51.7369\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 49.0476 - val_loss: 51.5840\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 48.9680 - val_loss: 51.5746\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 48.9609 - val_loss: 51.4212\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 49.0010 - val_loss: 51.5632\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 48.9155 - val_loss: 51.6200\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 49.0210 - val_loss: 51.5594\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 48.8930 - val_loss: 51.4098\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 49.0454 - val_loss: 51.6212\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 48.8955 - val_loss: 51.5405\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 48.9370 - val_loss: 52.4375\n",
      "7 DH3\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 7s 16us/step - loss: 57.4213 - val_loss: 53.2676\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 7s 15us/step - loss: 51.4194 - val_loss: 51.2547\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 48.5443 - val_loss: 50.4831\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 47.5025 - val_loss: 51.0129\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 47.4149 - val_loss: 49.0783\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 46.5128 - val_loss: 48.9512\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 46.6059 - val_loss: 49.1350\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 46.2141 - val_loss: 48.6916\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 46.2119 - val_loss: 49.4335\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 46.0964 - val_loss: 48.4436\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.8668 - val_loss: 49.6108\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 46.0243 - val_loss: 49.2248\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.6023 - val_loss: 48.8358\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 46.3528 - val_loss: 48.7905\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.5834 - val_loss: 48.3070\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.6174 - val_loss: 48.4258\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 45.3100 - val_loss: 48.0891\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.3426 - val_loss: 48.9714\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.3486 - val_loss: 48.1299\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.1432 - val_loss: 48.1150\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 7s 15us/step - loss: 45.6165 - val_loss: 48.5822\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.0652 - val_loss: 48.2193\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.3373 - val_loss: 49.3650\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.0310 - val_loss: 48.5128\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.2092 - val_loss: 48.1913\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.0533 - val_loss: 47.9629\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.0498 - val_loss: 48.1125\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 45.1665 - val_loss: 47.7723\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.9254 - val_loss: 49.1372\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.1240 - val_loss: 48.5579\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.8553 - val_loss: 48.0506\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.0706 - val_loss: 47.8665\n",
      "8 DH4\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 8s 18us/step - loss: 57.8398 - val_loss: 54.5969\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 53.5221 - val_loss: 50.7740\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 48.6296 - val_loss: 49.2047\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 47.5305 - val_loss: 50.6429\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 46.5977 - val_loss: 48.8818\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.8440 - val_loss: 48.5012\n",
      "Epoch 7/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 45.8651 - val_loss: 48.5608\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 45.2330 - val_loss: 48.7154\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 45.3490 - val_loss: 47.8234\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.9753 - val_loss: 47.8893\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.9570 - val_loss: 48.1522\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.8833 - val_loss: 48.3411\n",
      "Epoch 13/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.6207 - val_loss: 48.1592\n",
      "Epoch 14/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.7723 - val_loss: 47.7057\n",
      "Epoch 15/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.4146 - val_loss: 47.9417\n",
      "Epoch 16/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.7289 - val_loss: 47.3582\n",
      "Epoch 17/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.3202 - val_loss: 48.1967\n",
      "Epoch 18/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.4743 - val_loss: 47.6891\n",
      "Epoch 19/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 44.3578 - val_loss: 47.7182\n",
      "Epoch 20/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.2570 - val_loss: 47.7830\n",
      "Epoch 21/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 44.4768 - val_loss: 47.8026\n",
      "Epoch 22/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.1065 - val_loss: 47.6689\n",
      "Epoch 23/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.4243 - val_loss: 47.7856\n",
      "Epoch 24/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.0385 - val_loss: 48.0181\n",
      "Epoch 25/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.3633 - val_loss: 48.8598\n",
      "Epoch 26/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.0880 - val_loss: 47.7393\n",
      "Epoch 27/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.1486 - val_loss: 48.8844\n",
      "Epoch 28/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 44.1076 - val_loss: 47.9651\n",
      "Epoch 29/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 44.0088 - val_loss: 47.5950\n",
      "Epoch 30/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 44.3277 - val_loss: 47.8132\n",
      "Epoch 31/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 43.8958 - val_loss: 47.6543\n",
      "Epoch 32/32\n",
      "449885/449885 [==============================] - 5s 11us/step - loss: 44.2190 - val_loss: 48.0481\n",
      "9 DH11\n",
      "Train on 449885 samples, validate on 82892 samples\n",
      "Epoch 1/32\n",
      "449885/449885 [==============================] - 8s 17us/step - loss: 54.9594 - val_loss: 47.7901\n",
      "Epoch 2/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 46.3183 - val_loss: 44.0695\n",
      "Epoch 3/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 43.0701 - val_loss: 43.1623\n",
      "Epoch 4/32\n",
      "449885/449885 [==============================] - 6s 12us/step - loss: 42.6692 - val_loss: 43.2609\n",
      "Epoch 5/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 42.2476 - val_loss: 42.4494\n",
      "Epoch 6/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 42.0380 - val_loss: 42.5542\n",
      "Epoch 7/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449885/449885 [==============================] - 6s 13us/step - loss: 42.3771 - val_loss: 42.9830\n",
      "Epoch 8/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 41.6436 - val_loss: 42.7469\n",
      "Epoch 9/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 41.8047 - val_loss: 42.5463\n",
      "Epoch 10/32\n",
      "449885/449885 [==============================] - 6s 13us/step - loss: 41.4442 - val_loss: 42.3194\n",
      "Epoch 11/32\n",
      "449885/449885 [==============================] - 6s 14us/step - loss: 41.5065 - val_loss: 42.4468\n",
      "Epoch 12/32\n",
      "449885/449885 [==============================] - 5s 12us/step - loss: 41.3945 - val_loss: 42.0062\n",
      "Epoch 13/32\n",
      "415744/449885 [==========================>...] - ETA: 0s - loss: 41.3350"
     ]
    }
   ],
   "source": [
    "maes = {}\n",
    "for i in range(len(longs_np)):\n",
    "    print(i, longs_np[i])\n",
    "    sensor, mae = train_and_test_sensor(i)\n",
    "    maes[sensor] = mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = pd.Series(maes, name='MAE').sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AP3      9.689523\n",
       "DH8     39.415958\n",
       "DH11    41.442673\n",
       "AP5     41.738685\n",
       "DH6     42.791773\n",
       "DH10    43.391440\n",
       "DH9     43.446805\n",
       "DH7     46.676140\n",
       "AP1     47.536166\n",
       "DH3     47.866472\n",
       "DH4     48.048144\n",
       "DH2     50.818569\n",
       "AP4     51.337906\n",
       "DH1     51.957808\n",
       "DH5     52.437509\n",
       "AP7     56.508694\n",
       "AP6     57.921463\n",
       "Name: MAE, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
