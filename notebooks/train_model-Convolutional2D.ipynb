{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train and test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:25.326356Z",
     "start_time": "2019-04-23T19:36:24.972352Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import pandas_profiling\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "df = (feather.read_dataframe('/home/SHARED/SOLAR/data/oahu_min.feather')\n",
    "             .set_index('Datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:25.481743Z",
     "start_time": "2019-04-23T19:36:25.429394Z"
    }
   },
   "outputs": [],
   "source": [
    "# We load the info of the sensors to extract the longitude information\n",
    "info = pd.read_csv('/home/SHARED/SOLAR/data/info.csv')\n",
    "\n",
    "info.Location = info.Location.apply(lambda x: (x[:2] + x[-2:]).replace('_', ''))\n",
    "info.index = info.Location\n",
    "# Sorted longitudes\n",
    "longs = info['Longitude'].sort_values(ascending=False)\n",
    "\n",
    "# We drop two sensors (they are different compared to the other 17, since they are \"tilted\")\n",
    "df.drop('GT_AP6', inplace=True, axis=1)\n",
    "df.drop('GT_DH1', inplace=True, axis=1)\n",
    "\n",
    "# Just some auxiliar code to homogeneize name of sensors across different tables\n",
    "homogen_name = lambda x: x[-4:].replace('_', '')\n",
    "df.columns = [homogen_name(x) for x in df.columns.values.tolist()]\n",
    "\n",
    "# Finally, we sort the data according to sensor's longitude\n",
    "df = df[longs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:26.006407Z",
     "start_time": "2019-04-23T19:36:26.004129Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas_profiling.ProfileReport(df,bins=20,correlation_threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:26.545313Z",
     "start_time": "2019-04-23T19:36:26.541534Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15722324/sliding-window-in-numpy\n",
    "def window_stack_forward(a, stepsize=1, width=3):\n",
    "    return np.hstack( a[i:1+i-width or None:stepsize] for i in range(0, width) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:27.232032Z",
     "start_time": "2019-04-23T19:36:27.229007Z"
    }
   },
   "outputs": [],
   "source": [
    "# I feel this function can also be done for pd.DataFrame\n",
    "def window_stack(a, width=3):\n",
    "    n = a.shape[0]\n",
    "    return np.hstack(list(a[(width-1-i):(n-i)] for i in range(0, width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:28.211438Z",
     "start_time": "2019-04-23T19:36:27.711838Z"
    }
   },
   "outputs": [],
   "source": [
    "# In pandas 0.24, use df.to_numpy() instead of df.values. Also care with non-numeric columns\n",
    "width = 61\n",
    "a = window_stack(df.to_numpy(), width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:28.305817Z",
     "start_time": "2019-04-23T19:36:28.297513Z"
    }
   },
   "outputs": [],
   "source": [
    "times   = [ ('t' if not idx else 't-{:d}'.format(idx)) for idx in range(width) ]\n",
    "columns = pd.MultiIndex.from_product((times, df.columns), names=('time', 'location'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:28.807129Z",
     "start_time": "2019-04-23T19:36:28.801778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert back to DataFrame, just for convenience of having indexes\n",
    "df_roll = pd.DataFrame(a, index=df.index[width-1:], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:30.332605Z",
     "start_time": "2019-04-23T19:36:29.371724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split target (time t) and variables (times t-1 to t-width+1)\n",
    "y = df_roll['t']\n",
    "X = df_roll.drop(columns='t', level='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:30.358929Z",
     "start_time": "2019-04-23T19:36:30.334541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split train-test, approximately 12 and 4 months respectively\n",
    "X_train, X_test = X[:'2011-07-31'], X['2011-08-01':]\n",
    "y_train, y_test = y[:'2011-07-31'], y['2011-08-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:31.123255Z",
     "start_time": "2019-04-23T19:36:31.119568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(449885, 1020)\n",
      "(82892, 1020)\n",
      "(449885, 17)\n",
      "(82892, 17)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are going to select more than one time slice as inputs. In this case X is a numpy array of rank 3.\n",
    "\n",
    "X : (item,sensor,time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:34.460788Z",
     "start_time": "2019-04-23T19:36:34.455970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sensors considered: 17\n",
      "Chosen times slices as features: ['t-1', 't-2', 't-3', 't-4', 't-5', 't-6', 't-7', 't-8', 't-9']\n",
      "Number of time slices considered: 9\n"
     ]
    }
   ],
   "source": [
    "longs_np = longs.index.to_numpy()\n",
    "n_sensors = len(longs_np)\n",
    "print('Number of sensors considered:',n_sensors)\n",
    "\n",
    "chosen_times = times[1:10:1]\n",
    "print('Chosen times slices as features:',chosen_times)\n",
    "print('Number of time slices considered:',len(chosen_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:36:21.094500Z",
     "start_time": "2019-04-23T19:36:21.039597Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape, Add, Multiply, Subtract, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, LocallyConnected1D, Conv1D, UpSampling1D, MaxPooling1D, Dot, Concatenate\n",
    "from keras.layers import LocallyConnected2D, Conv2D\n",
    "from keras import backend as K\n",
    "from clr import CyclicLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:42:34.224227Z",
     "start_time": "2019-04-23T19:42:33.970560Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolutional 1D with last time slice\n",
    "\n",
    "First we preprocess the dataset (for the moment, we'll just use as features the t-1 values at each sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:29.023584Z",
     "start_time": "2019-04-23T12:43:29.019131Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We only use the previous timestep as features\n",
    "X_tr1 = X_train['t-1']\n",
    "y_tr1 = y_train\n",
    "\n",
    "X_te1 = X_test['t-1']\n",
    "y_te1 = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, in order to use a 1D convolution, we are going to sort the sensors. For the initial test, we'll just sort them by longitude (from East to West). That way, nearer sensors are in close positions in the tensor, so the 1D convolution may extract useful correlations.\n",
    "\n",
    "Note: many other possible ordenations of the sensors could be added as new channels in the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:30.876001Z",
     "start_time": "2019-04-23T12:43:30.285662Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidgg/anaconda3/envs/solar/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# We load the info of the sensors to extract the longitude information\n",
    "info = pd.read_csv('/home/SHARED/SOLAR/data/info.csv')\n",
    "\n",
    "info.Location = info.Location.apply(lambda x: (x[:2] + x[-2:]).replace('_', ''))\n",
    "info.index = info.Location\n",
    "# Sorted longitudes\n",
    "longs = info['Longitude'].sort_values(ascending=False)\n",
    "\n",
    "# We drop two sensors (they are different compared to the other 17, since they are \"tilted\")\n",
    "X_tr1.drop('GT_AP6', inplace=True, axis=1)\n",
    "y_tr1.drop('GT_AP6', inplace=True, axis=1)\n",
    "X_tr1.drop('GT_DH1', inplace=True, axis=1)\n",
    "y_tr1.drop('GT_DH1', inplace=True, axis=1)\n",
    "X_te1.drop('GT_AP6', inplace=True, axis=1)\n",
    "y_te1.drop('GT_AP6', inplace=True, axis=1)\n",
    "X_te1.drop('GT_DH1', inplace=True, axis=1)\n",
    "y_te1.drop('GT_DH1', inplace=True, axis=1)\n",
    "\n",
    "# Just some auxiliar code to homogeneize name of sensors across different tables\n",
    "homogen_name = lambda x: x[-4:].replace('_', '')\n",
    "X_tr1.columns = [homogen_name(x) for x in X_tr1.columns.values.tolist()]\n",
    "y_tr1.columns = [homogen_name(x) for x in y_tr1.columns.values.tolist()]\n",
    "X_te1.columns = [homogen_name(x) for x in X_te1.columns.values.tolist()]\n",
    "y_te1.columns = [homogen_name(x) for x in y_te1.columns.values.tolist()]\n",
    "\n",
    "\n",
    "# Finally, we sort the data according to sensor's longitude\n",
    "X_tr1_1 = X_tr1[longs.index]\n",
    "y_tr1_1 = y_tr1[longs.index]\n",
    "X_te1_1 = X_te1[longs.index]\n",
    "y_te1_1 = y_te1[longs.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we specify which sensor do we want to predict and test.\n",
    "\n",
    "(In the future, we need to discuss how are we going to predict, if just by looping over each sensor, or just give a vectorial prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T14:42:03.128524Z",
     "start_time": "2019-04-23T14:42:03.123293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:07.469574Z",
     "start_time": "2019-04-23T12:43:07.463305Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape, Add, Multiply, Subtract, Dropout\n",
    "# from keras.layers import Conv2D, MaxPooling2D, LocallyConnected1D, Conv1D, UpSampling1D, MaxPooling1D, Dot, Concatenate\n",
    "\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Model architecture is defined below.\n",
    "\n",
    "Some highlights:\n",
    "* Locally connected works better than pure convolutional at the first layers (probably because the sensors at not located in a uniform grid)\n",
    "* Trick to improve acc: add a final layer combining the convolutional prediction with the persistance prediction, so in case the input is \"strange\", the model could learn to output the persistance prediction (i.e., the previous time-step), which is somewhat reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:38.963052Z",
     "start_time": "2019-04-23T12:43:38.950629Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_model_sensor(index_sensor, n_sensors=17):\n",
    "    ''' Returns a model using all the sensors to predict index_sensor '''\n",
    "    xin = Input(shape=(n_sensors,1), name='main_input')\n",
    "    x = LocallyConnected1D(8, 7, data_format = 'channels_last', padding='valid')(xin)\n",
    "    x = Activation('relu')(x)\n",
    "    x = LocallyConnected1D(16, 5, data_format = 'channels_last', padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, data_format = 'channels_last', padding='causal')(x)\n",
    "    xl = Flatten()(x)\n",
    "    xl = Dropout(0.2)(xl)\n",
    "    xo = Dense(1)(xl)\n",
    "\n",
    "    # use date info here?\n",
    "    xinf = Flatten()(xin)\n",
    "    s  = Dense(5)(xinf)\n",
    "    s = Activation('tanh')(s)\n",
    "    s = Dense(2)(s)\n",
    "    s = Activation('softmax')(s)\n",
    "\n",
    "    # sort of residual connection\n",
    "    xin_0 = Activation('relu')(xin)\n",
    "    xin_1 = Lambda(lambda x : x[:,index_sensor,:])(xin_0)\n",
    "    xo_m = Dot(axes=1)([Concatenate()([xo,xin_1]), s])\n",
    "    xo_m = Activation('relu')(xo_m)\n",
    "\n",
    "    model = Model(inputs=[xin], outputs=[xo_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:39.991849Z",
     "start_time": "2019-04-23T12:43:39.970984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "lr = 0.0001\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "# We add a callback to log metrics and another one to schedule the learning rate\n",
    "\n",
    "#see clr.py in this same folder\n",
    "from clr import CyclicLR\n",
    "\n",
    "c1 = keras.callbacks.BaseLogger(stateful_metrics=None)\n",
    "c2 = CyclicLR(step_size=250, base_lr=lr)\n",
    "c3 = keras.callbacks.History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we are ready to train. The below configuration should take 2 minutes in a 16 core CPU\n",
    "(no GPU needed). We are using a huge batch-size to speed up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:42.886599Z",
     "start_time": "2019-04-23T12:43:42.881097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_sensors = 17\n",
    "\n",
    "def to_array(sensor='AP5', val=0.1):\n",
    "    ''' Converts dataframe to numpy array for predicting any given sensor. val specifies the fraction\n",
    "    of training samples to be used as validation. '''\n",
    "    X_tr1_1_np = X_tr1_1.values\n",
    "    y_tr1_1_np = y_tr1_1[sensor].values\n",
    "    \n",
    "    #val_idx = int((1 - val)*len(y_tr1_1_np))\n",
    "\n",
    "    X_te1_1_np = X_te1_1.values\n",
    "    y_te1_1_np = y_te1_1[sensor].values\n",
    "    \n",
    "    #return X_tr1_1_np[:val_idx], y_tr1_1_np[:val_idx], X_tr1_1_np[val_idx:], y_tr1_1_np[val_idx:], X_te1_1_np, y_te1_1_np\n",
    "    return X_tr1_1_np, y_tr1_1_np, X_te1_1_np, y_te1_1_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:43:44.133881Z",
     "start_time": "2019-04-23T12:43:44.131216Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1 << 11   # as big as possible so we can explore many models\n",
    "epochs = 1 << 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:44:01.101770Z",
     "start_time": "2019-04-23T12:44:00.724918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "longs_np = longs.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T12:44:03.227212Z",
     "start_time": "2019-04-23T12:44:03.217516Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_sensor(id_sensor=4):\n",
    "    X_tr, y_tr, X_te, y_te = to_array(sensor=longs_np[id_sensor])\n",
    "    \n",
    "    \n",
    "    # Validation using TS split (just to obtain different MAE estimations, no hyperoptimization for the moment)\n",
    "    for tr_idx, va_idx in TimeSeriesSplit(n_splits=5).split(X_tr):\n",
    "        model = make_model_sensor(id_sensor, n_sensors=17)\n",
    "        model.compile(opt, loss='mean_absolute_error')\n",
    "        model.fit(np.atleast_3d(X_tr[tr_idx]), y_tr[tr_idx], batch_size=batch_size, epochs=epochs, validation_data=\n",
    "              (np.atleast_3d(X_tr[va_idx]),y_tr[va_idx]), callbacks=[c2, c3], verbose=0)\n",
    "        print('MAE_val ', c3.history['val_loss'][-1])\n",
    "    \n",
    "    # Testing\n",
    "    model = make_model_sensor(id_sensor, n_sensors=17)\n",
    "    model.compile(opt, loss='mean_absolute_error')\n",
    "    model.fit(np.atleast_3d(X_tr), y_tr, batch_size=batch_size, epochs=epochs, validation_data=\n",
    "              (np.atleast_3d(X_te),y_te), callbacks=[c2, c3], verbose=0)\n",
    "    \n",
    "    print('MAE_test ', c3.history['val_loss'][-1])\n",
    "    return longs_np[id_sensor], c3.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T14:28:11.698489Z",
     "start_time": "2019-04-23T12:44:04.812219Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AP7\n",
      "MAE_val  75.80979736120578\n",
      "MAE_val  51.144046569770296\n",
      "MAE_val  23.682809347927705\n",
      "MAE_val  48.48229516641444\n",
      "MAE_val  77.36800727015067\n",
      "MAE_test  56.7753012248093\n",
      "1 AP6\n",
      "MAE_val  74.61921432023048\n",
      "MAE_val  49.396017061638695\n",
      "MAE_val  22.354893112182616\n",
      "MAE_val  45.92701640405692\n",
      "MAE_val  76.46450659658674\n",
      "MAE_test  56.88833060529433\n",
      "2 AP4\n",
      "MAE_val  66.64517080325065\n",
      "MAE_val  44.6123261167234\n",
      "MAE_val  22.224348633040396\n",
      "MAE_val  46.57306654620193\n",
      "MAE_val  70.25389557251026\n",
      "MAE_test  53.94465374119383\n",
      "3 AP3\n",
      "MAE_val  53.7360220670414\n",
      "MAE_val  40.76224770307509\n",
      "MAE_val  21.362736480505344\n",
      "MAE_val  73.05565114166914\n",
      "MAE_val  46.59384850912657\n",
      "MAE_test  6.656649084376051\n",
      "4 AP5\n",
      "MAE_val  55.24970045057928\n",
      "MAE_val  36.355663239521924\n",
      "MAE_val  22.087352481755055\n",
      "MAE_val  37.961037127287135\n",
      "MAE_val  57.43544904833747\n",
      "MAE_test  41.17154105209058\n",
      "5 AP1\n",
      "MAE_val  53.48813401489421\n",
      "MAE_val  41.610838796882916\n",
      "MAE_val  20.758525616729365\n",
      "MAE_val  41.51859642320799\n",
      "MAE_val  60.26669131750933\n",
      "MAE_test  51.893634877690175\n",
      "6 DH5\n",
      "MAE_val  57.29523281021617\n",
      "MAE_val  44.95631776589173\n",
      "MAE_val  22.097038118622116\n",
      "MAE_val  58.38963878472371\n",
      "MAE_val  59.86065306082966\n",
      "MAE_test  49.663516282770665\n",
      "7 DH3\n",
      "MAE_val  56.15551522978943\n",
      "MAE_val  41.695836925773655\n",
      "MAE_val  21.07067036077426\n",
      "MAE_val  44.61714998085764\n",
      "MAE_val  69.94008512088722\n",
      "MAE_test  50.24999291806046\n",
      "8 DH4\n",
      "MAE_val  57.03879248963338\n",
      "MAE_val  41.81020250848205\n",
      "MAE_val  20.58031594662654\n",
      "MAE_val  80.65033825025954\n",
      "MAE_val  62.94722596578144\n",
      "MAE_test  48.43372359733948\n",
      "9 DH11\n",
      "MAE_val  51.35145165351474\n",
      "MAE_val  36.128722275427336\n",
      "MAE_val  20.718283615946564\n",
      "MAE_val  38.69213656211697\n",
      "MAE_val  54.965672217904604\n",
      "MAE_test  40.680962499743615\n",
      "10 DH2\n",
      "MAE_val  65.52352382722482\n",
      "MAE_val  47.42712075406034\n",
      "MAE_val  22.381900362034354\n",
      "MAE_val  45.257302432906584\n",
      "MAE_val  65.48225004310878\n",
      "MAE_test  51.12946275584873\n",
      "11 DH10\n",
      "MAE_val  52.04082090065215\n",
      "MAE_val  36.190239527511956\n",
      "MAE_val  21.169756854065323\n",
      "MAE_val  52.10983741089795\n",
      "MAE_val  60.50605036512599\n",
      "MAE_test  43.26168022262528\n",
      "12 DH8\n",
      "MAE_val  50.15223863944799\n",
      "MAE_val  35.42386604814724\n",
      "MAE_val  19.3684672676002\n",
      "MAE_val  37.72427639786379\n",
      "MAE_val  55.4202511810433\n",
      "MAE_test  40.13945658812219\n",
      "13 DH6\n",
      "MAE_val  50.76259777467399\n",
      "MAE_val  38.30799087303641\n",
      "MAE_val  20.780452662395597\n",
      "MAE_val  50.36524102119052\n",
      "MAE_val  59.98051964515272\n",
      "MAE_test  43.37830131794733\n",
      "14 DH7\n",
      "MAE_val  61.80947955485455\n",
      "MAE_val  42.680288692041465\n",
      "MAE_val  21.494445962189896\n",
      "MAE_val  42.520794908071075\n",
      "MAE_val  62.52751524788266\n",
      "MAE_test  45.890827926505416\n",
      "15 DH9\n",
      "MAE_val  57.69496536672012\n",
      "MAE_val  39.67647462591265\n",
      "MAE_val  21.154198369688633\n",
      "MAE_val  44.21872705321484\n",
      "MAE_val  66.29857143621346\n",
      "MAE_test  44.516582787960175\n",
      "16 DH1\n",
      "MAE_val  67.35860088911512\n",
      "MAE_val  46.34590546226069\n",
      "MAE_val  22.069913599563925\n",
      "MAE_val  44.927284179984575\n",
      "MAE_val  66.5199438832445\n",
      "MAE_test  50.18902113883702\n"
     ]
    }
   ],
   "source": [
    "maes = {}\n",
    "for i in range(len(longs_np)):\n",
    "    print(i, longs_np[i])\n",
    "    sensor, mae = train_and_test_sensor(i)\n",
    "    maes[sensor] = mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T14:29:13.525875Z",
     "start_time": "2019-04-23T14:29:13.520942Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maes = pd.Series(maes, name='MAE').sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T14:29:15.071965Z",
     "start_time": "2019-04-23T14:29:15.058928Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AP3      6.656649\n",
       "DH8     40.139457\n",
       "DH11    40.680962\n",
       "AP5     41.171541\n",
       "DH10    43.261680\n",
       "DH6     43.378301\n",
       "DH9     44.516583\n",
       "DH7     45.890828\n",
       "DH4     48.433724\n",
       "DH5     49.663516\n",
       "DH1     50.189021\n",
       "DH3     50.249993\n",
       "DH2     51.129463\n",
       "AP1     51.893635\n",
       "AP4     53.944654\n",
       "AP7     56.775301\n",
       "AP6     56.888331\n",
       "Name: MAE, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional 1D over time (longitude,time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:47:33.170921Z",
     "start_time": "2019-04-23T19:47:33.132134Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "lr = 0.0001\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "# We add a callback to log metrics and another one to schedule the learning rate\n",
    "\n",
    "#see clr.py in this same folder\n",
    "from clr import CyclicLR\n",
    "\n",
    "c1 = keras.callbacks.BaseLogger(stateful_metrics=None)\n",
    "c2 = CyclicLR(step_size=250, base_lr=lr)\n",
    "c3 = keras.callbacks.History()\n",
    "c4 = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "batch_size = 1 << 10   # as big as possible so we can explore many models\n",
    "epochs = 1 << 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:47:33.441593Z",
     "start_time": "2019-04-23T19:47:33.427689Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_array(chosen_times, sensor='AP5', sort_time = True):\n",
    "    ''' Converts dataframe to numpy array for predicting any given sensor. \n",
    "\n",
    "    Output shape: (items, time_slice, sensor) 'channels_last'\n",
    "    Output shape Y : (items)\n",
    "    '''\n",
    "    \n",
    "    X_train_np = []\n",
    "    for time in chosen_times:\n",
    "        X_train_np.append(X_train[time].to_numpy().reshape(-1,n_sensors,1))\n",
    "    X_train_np = np.swapaxes(np.concatenate(X_train_np,axis=2),1,2)\n",
    "\n",
    "    y_train_np = y_train[sensor].to_numpy()\n",
    "\n",
    "    X_test_np = []\n",
    "    for time in chosen_times:\n",
    "        X_test_np.append(X_test[time].to_numpy().reshape(-1,n_sensors,1))\n",
    "    X_test_np = np.swapaxes(np.concatenate(X_test_np,axis=2),1,2)\n",
    "    y_test_np = y_test[sensor].to_numpy()\n",
    "    \n",
    "    if sort_time:\n",
    "        X_train_np = X_train_np[:,::-1]\n",
    "        X_test_np = X_test_np[:,::-1]\n",
    "        \n",
    "    assert X_train_np.shape[1] == len(chosen_times)\n",
    "    assert X_test_np.shape[1] == len(chosen_times)\n",
    "    #return X_tr1_1_np[:val_idx], y_tr1_1_np[:val_idx], X_tr1_1_np[val_idx:], y_tr1_1_np[val_idx:], X_te1_1_np, y_te1_1_np\n",
    "    return X_train_np, y_train_np, X_test_np, y_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:47:33.752974Z",
     "start_time": "2019-04-23T19:47:33.736538Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model_sensor_conv1d_time(index_sensor, n_sensors=17, time_slices = 2):\n",
    "    ''' Returns a model using all the sensors to predict index_sensor '''\n",
    "    xin = Input(shape=(time_slices,n_sensors), name='main_input')\n",
    "    '''\n",
    "    When using this layer as the first layer in a model, provide an input_shape argument \n",
    "    (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) \n",
    "    for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\", \n",
    "    or (None, 128) for variable-length sequences with 128 features per step.\n",
    "    '''\n",
    "    # The convolution is across time, and each sensor is a channel\n",
    "    x = LocallyConnected1D(20, 3, data_format = 'channels_last', padding='valid')(xin)\n",
    "    x = Activation('relu')(x)\n",
    "    x = LocallyConnected1D(12, 3, data_format = 'channels_last', padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, data_format = 'channels_last', padding='causal')(x)\n",
    "    xl = Flatten()(x)\n",
    "    xl = Dropout(0.2)(xl)\n",
    "    xo = Dense(1)(xl)\n",
    "\n",
    "    # use date info here?\n",
    "    xinf = Flatten()(xin)\n",
    "    s  = Dense(5)(xinf)\n",
    "    s = Activation('tanh')(s)\n",
    "    s = Dense(2)(s)\n",
    "    s = Activation('softmax')(s)\n",
    "\n",
    "    # sort of residual connection, we only take the last timestamp of the given sensor x[:,index_sensor,-1:]\n",
    "    xin_0 = Activation('relu')(xin)\n",
    "    xin_1 = Lambda(lambda x : x[:,index_sensor,-1:])(xin_0)\n",
    "#     xin_1 = Flatten()(xin_1)\n",
    "    xo_m = Dot(axes=1)([Concatenate()([xo,xin_1]), s])\n",
    "    xo_m = Activation('relu')(xo_m)\n",
    "\n",
    "    model = Model(inputs=[xin], outputs=[xo_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:47:37.514451Z",
     "start_time": "2019-04-23T19:47:37.292131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 13,067\n",
      "Trainable params: 13,067\n",
      "Non-trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "model = make_model_sensor_conv1d_time(0, n_sensors=n_sensors,time_slices=len(chosen_times))\n",
    "# model.summary()\n",
    "trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "non_trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:51:06.532366Z",
     "start_time": "2019-04-23T19:51:06.517561Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_test_sensor(id_sensor=4, verbose = 0):\n",
    "    mae_val_list = []\n",
    "    X_tr, y_tr, X_te, y_te = to_array(chosen_times,sensor=longs_np[id_sensor])\n",
    "    # Validation using TS split (just to obtain different MAE estimations, no hyperoptimization for the moment)\n",
    "    for tr_idx, va_idx in TimeSeriesSplit(n_splits=5).split(X_tr):\n",
    "        model = make_model_sensor_conv1d_time(id_sensor, n_sensors=n_sensors,time_slices=len(chosen_times))\n",
    "        model.compile(opt, loss='mean_absolute_error')\n",
    "        \n",
    "        model.fit(np.atleast_3d(X_tr[tr_idx]), y_tr[tr_idx], batch_size=batch_size, epochs=epochs, validation_data=\n",
    "              (np.atleast_3d(X_tr[va_idx]),y_tr[va_idx]), callbacks=[c2, c3,c4], verbose=verbose)\n",
    "        mae_val_list.append(c3.history['val_loss'][-1])\n",
    "        print('MAE_val ', c3.history['val_loss'][-1])\n",
    "    \n",
    "#     # Testing\n",
    "#     model = make_model_sensor_conv1d_time(id_sensor, n_sensors=n_sensors,time_slices=len(chosen_times))\n",
    "#     model.compile(opt, loss='mean_absolute_error')\n",
    "#     model.fit(np.atleast_3d(X_tr), y_tr, batch_size=batch_size, epochs=epochs, validation_data=\n",
    "#               (np.atleast_3d(X_te),y_te), callbacks=[c2, c3,c4], verbose=0)\n",
    "    \n",
    "#     print('MAE_test ', c3.history['val_loss'][-1])\n",
    "    return longs_np[id_sensor], np.array(mae_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 AP7\n",
    "\n",
    "MAE_val  75.80979736120578\n",
    "\n",
    "MAE_val  51.144046569770296\n",
    "\n",
    "MAE_val  23.682809347927705\n",
    "\n",
    "MAE_val  48.48229516641444\n",
    "\n",
    "MAE_val  77.36800727015067\n",
    "\n",
    "MAE_test  56.7753012248093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T19:50:48.604832Z",
     "start_time": "2019-04-23T19:47:45.067829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AP7\n",
      "Train on 74985 samples, validate on 74980 samples\n",
      "Epoch 1/32\n",
      "74985/74985 [==============================] - 2s 20us/step - loss: 136.8726 - val_loss: 116.0543\n",
      "Epoch 2/32\n",
      "74985/74985 [==============================] - 1s 14us/step - loss: 110.1401 - val_loss: 102.9457\n",
      "Epoch 3/32\n",
      "74985/74985 [==============================] - 1s 14us/step - loss: 101.5190 - val_loss: 93.2281\n",
      "Epoch 4/32\n",
      "74985/74985 [==============================] - 1s 13us/step - loss: 93.5487 - val_loss: 102.7276\n",
      "Epoch 5/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 88.5797 - val_loss: 82.2268\n",
      "Epoch 6/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 84.8655 - val_loss: 84.3284\n",
      "Epoch 7/32\n",
      "74985/74985 [==============================] - 1s 14us/step - loss: 83.8947 - val_loss: 79.8804\n",
      "Epoch 8/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 84.1746 - val_loss: 79.3444\n",
      "Epoch 9/32\n",
      "74985/74985 [==============================] - 1s 14us/step - loss: 84.3482 - val_loss: 80.6858\n",
      "Epoch 10/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 86.3478 - val_loss: 79.7483\n",
      "Epoch 11/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 85.0587 - val_loss: 81.4960\n",
      "Epoch 12/32\n",
      "74985/74985 [==============================] - 1s 16us/step - loss: 84.1828 - val_loss: 78.6818\n",
      "Epoch 13/32\n",
      "74985/74985 [==============================] - 1s 16us/step - loss: 82.5736 - val_loss: 77.5328\n",
      "Epoch 14/32\n",
      "74985/74985 [==============================] - 1s 13us/step - loss: 81.5487 - val_loss: 78.6241\n",
      "Epoch 15/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 81.8924 - val_loss: 78.9072\n",
      "Epoch 16/32\n",
      "74985/74985 [==============================] - 1s 15us/step - loss: 82.7733 - val_loss: 78.1704\n",
      "Epoch 17/32\n",
      "74985/74985 [==============================] - 1s 13us/step - loss: 84.1363 - val_loss: 85.4355\n",
      "Epoch 18/32\n",
      "74985/74985 [==============================] - 1s 14us/step - loss: 83.8291 - val_loss: 77.8542\n",
      "MAE_val  77.8541719190214\n",
      "Train on 149965 samples, validate on 74980 samples\n",
      "Epoch 1/32\n",
      "149965/149965 [==============================] - 2s 14us/step - loss: 111.3182 - val_loss: 64.4231\n",
      "Epoch 2/32\n",
      "149965/149965 [==============================] - 2s 14us/step - loss: 93.9877 - val_loss: 59.4314\n",
      "Epoch 3/32\n",
      "149965/149965 [==============================] - 2s 14us/step - loss: 91.9764 - val_loss: 55.0489\n",
      "Epoch 4/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 88.3921 - val_loss: 54.5266\n",
      "Epoch 5/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 85.6162 - val_loss: 53.8026\n",
      "Epoch 6/32\n",
      "149965/149965 [==============================] - 2s 12us/step - loss: 85.7062 - val_loss: 53.5325\n",
      "Epoch 7/32\n",
      "149965/149965 [==============================] - 2s 12us/step - loss: 85.9522 - val_loss: 52.9010\n",
      "Epoch 8/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 85.0734 - val_loss: 53.5305\n",
      "Epoch 9/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 85.2849 - val_loss: 57.7197\n",
      "Epoch 10/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 85.9558 - val_loss: 59.5210\n",
      "Epoch 11/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 85.4229 - val_loss: 52.9923\n",
      "Epoch 12/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 84.7118 - val_loss: 52.5347\n",
      "Epoch 13/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 85.2173 - val_loss: 56.4427\n",
      "Epoch 14/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 86.0920 - val_loss: 56.1527\n",
      "Epoch 15/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 84.7097 - val_loss: 52.6619\n",
      "Epoch 16/32\n",
      "149965/149965 [==============================] - 2s 13us/step - loss: 84.9775 - val_loss: 56.0426\n",
      "Epoch 17/32\n",
      "149965/149965 [==============================] - 2s 12us/step - loss: 86.2055 - val_loss: 57.6370\n",
      "MAE_val  57.63698660605556\n",
      "Train on 224945 samples, validate on 74980 samples\n",
      "Epoch 1/32\n",
      "224945/224945 [==============================] - 3s 14us/step - loss: 123.7457 - val_loss: 46.9217\n",
      "Epoch 2/32\n",
      "224945/224945 [==============================] - 2s 11us/step - loss: 96.2869 - val_loss: 26.4015\n",
      "Epoch 3/32\n",
      "224945/224945 [==============================] - 3s 12us/step - loss: 78.2102 - val_loss: 25.2914\n",
      "Epoch 4/32\n",
      "224945/224945 [==============================] - 2s 11us/step - loss: 76.7903 - val_loss: 26.5068\n",
      "Epoch 5/32\n",
      "224945/224945 [==============================] - 3s 12us/step - loss: 76.0584 - val_loss: 24.5385\n",
      "Epoch 6/32\n",
      "224945/224945 [==============================] - 3s 12us/step - loss: 75.1979 - val_loss: 27.5786\n",
      "Epoch 7/32\n",
      "224945/224945 [==============================] - 3s 12us/step - loss: 75.5638 - val_loss: 25.3631\n",
      "Epoch 8/32\n",
      "224945/224945 [==============================] - 3s 12us/step - loss: 74.9310 - val_loss: 26.1053\n",
      "Epoch 9/32\n",
      "224945/224945 [==============================] - 3s 11us/step - loss: 75.4532 - val_loss: 25.2019\n",
      "Epoch 10/32\n",
      "224945/224945 [==============================] - 3s 12us/step - loss: 74.8606 - val_loss: 24.9047\n",
      "MAE_val  24.90471947284087\n",
      "Train on 299925 samples, validate on 74980 samples\n",
      "Epoch 1/32\n",
      "299925/299925 [==============================] - 4s 14us/step - loss: 78.5620 - val_loss: 71.7007\n",
      "Epoch 2/32\n",
      "299925/299925 [==============================] - 3s 11us/step - loss: 65.2273 - val_loss: 51.1187\n",
      "Epoch 3/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 65.2210 - val_loss: 52.9409\n",
      "Epoch 4/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.9252 - val_loss: 53.5823\n",
      "Epoch 5/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 63.7468 - val_loss: 50.6252\n",
      "Epoch 6/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.7575 - val_loss: 49.9337\n",
      "Epoch 7/32\n",
      "299925/299925 [==============================] - 3s 11us/step - loss: 62.1587 - val_loss: 51.2042\n",
      "Epoch 8/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.7644 - val_loss: 51.6436\n",
      "Epoch 9/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.1732 - val_loss: 59.2351\n",
      "Epoch 10/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.8649 - val_loss: 49.4667\n",
      "Epoch 11/32\n",
      "299925/299925 [==============================] - 3s 11us/step - loss: 62.2583 - val_loss: 54.1199\n",
      "Epoch 12/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.7147 - val_loss: 49.6107\n",
      "Epoch 13/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.7120 - val_loss: 51.1332\n",
      "Epoch 14/32\n",
      "299925/299925 [==============================] - 3s 11us/step - loss: 61.5307 - val_loss: 53.6157\n",
      "Epoch 15/32\n",
      "299925/299925 [==============================] - 4s 12us/step - loss: 62.3736 - val_loss: 50.1357\n",
      "MAE_val  50.135655792187485\n",
      "Train on 374905 samples, validate on 74980 samples\n",
      "Epoch 1/32\n",
      "374905/374905 [==============================] - 5s 13us/step - loss: 80.2651 - val_loss: 83.3844\n",
      "Epoch 2/32\n",
      "374905/374905 [==============================] - 4s 12us/step - loss: 62.9867 - val_loss: 86.0082\n",
      "Epoch 3/32\n",
      "374905/374905 [==============================] - 4s 11us/step - loss: 62.2302 - val_loss: 78.7120\n",
      "Epoch 4/32\n",
      "374905/374905 [==============================] - 5s 12us/step - loss: 61.9913 - val_loss: 78.0601\n",
      "Epoch 5/32\n",
      "374905/374905 [==============================] - 4s 12us/step - loss: 60.6211 - val_loss: 85.9019\n",
      "Epoch 6/32\n",
      "374905/374905 [==============================] - 4s 12us/step - loss: 60.4793 - val_loss: 78.1577\n",
      "Epoch 7/32\n",
      "374905/374905 [==============================] - 4s 12us/step - loss: 61.1884 - val_loss: 78.3215\n",
      "Epoch 8/32\n",
      "374905/374905 [==============================] - 4s 12us/step - loss: 60.5895 - val_loss: 78.8933\n",
      "Epoch 9/32\n",
      "374905/374905 [==============================] - 4s 12us/step - loss: 60.2172 - val_loss: 79.1824\n",
      "MAE_val  79.18244126279947\n",
      "MAE list: [77.85417192 57.63698661 24.90471947 50.13565579 79.18244126]\n",
      "MAE: 57.94 (+/- 40.00)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(i, longs_np[i])\n",
    "sensor, mae_list = train_and_test_sensor(i,verbose=1)\n",
    "print('MAE list:',mae_list)\n",
    "print(\"MAE: %0.2f (+/- %0.2f)\" % (mae_list.mean(), mae_list.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:40.543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AP7\n",
      "MAE_val  99.6796622491131\n",
      "MAE_val  53.360176843870605\n"
     ]
    }
   ],
   "source": [
    "maes = {}\n",
    "maes_std = {}\n",
    "for i in range(len(longs_np)):\n",
    "    print(i, longs_np[i])\n",
    "    sensor, mae_list = train_and_test_sensor(i)\n",
    "    maes[sensor] = mae_list.mean() \n",
    "    maes_std[sensor] = mae_list.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:43.602Z"
    }
   },
   "outputs": [],
   "source": [
    "maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:43.978Z"
    }
   },
   "outputs": [],
   "source": [
    "maes_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:45.430Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_array(chosen_times, sensor='AP5', sort_time = True):\n",
    "    ''' Converts dataframe to numpy array for predicting any given sensor.\n",
    "    The X array has several time slices, given in chosen_times and\n",
    "    sorts them in ascending order.\n",
    "    Output shape X : (items, time_slice, sensor, 1), 'channels_last'\n",
    "    Output shape Y : (items)\n",
    "    '''\n",
    "    \n",
    "    X_train_np = []\n",
    "    for time in chosen_times:\n",
    "        X_train_np.append(X_train[time].to_numpy().reshape(-1,n_sensors,1))\n",
    "    X_train_np = np.swapaxes(np.concatenate(X_train_np,axis=2),1,2)\n",
    "\n",
    "    y_train_np = y_train[sensor].to_numpy()\n",
    "\n",
    "    \n",
    "    X_test_np = []\n",
    "    for time in chosen_times:\n",
    "        X_test_np.append(X_test[time].to_numpy().reshape(-1,n_sensors,1))\n",
    "    X_test_np = np.swapaxes(np.concatenate(X_test_np,axis=2),1,2)\n",
    "    y_test_np = y_test[sensor].to_numpy()\n",
    "    \n",
    "    \n",
    "    if sort_time:\n",
    "        X_train_np = np.expand_dims(X_train_np[:,::-1],axis=3)\n",
    "        X_test_np = np.expand_dims(X_test_np[:,::-1],axis=3)\n",
    "    \n",
    "    assert X_train_np.shape[1] == len(chosen_times)\n",
    "    assert X_test_np.shape[1] == len(chosen_times)\n",
    "    return X_train_np, y_train_np, X_test_np, y_test_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:50.547Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model_sensor_conv2d(index_sensor, n_sensors=17, time_slices = 2):\n",
    "    ''' Returns a model using all the sensors to predict index_sensor '''\n",
    "    xin = Input(shape=(time_slices,n_sensors,1), name='main_input')\n",
    "    '''\n",
    "    When using this layer as the first layer in a model, provide an input_shape argument \n",
    "    (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) \n",
    "    for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\", \n",
    "    or (None, 128) for variable-length sequences with 128 features per step.\n",
    "    '''\n",
    "    # The convolution is across time, and each sensor is a channel\n",
    "    x = LocallyConnected2D(10, 3, data_format = 'channels_last', padding='valid')(xin)\n",
    "    x = Activation('relu')(x)\n",
    "    x = LocallyConnected2D(5, 3, data_format = 'channels_last', padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(12, 3, data_format = 'channels_last', padding='valid')(x)\n",
    "    xl = Flatten()(x)\n",
    "    xl = Dropout(0.2)(xl)\n",
    "    xo = Dense(1)(xl)\n",
    "\n",
    "    # use date info here?\n",
    "    xinf = Flatten()(xin)\n",
    "    s  = Dense(5)(xinf)\n",
    "    s = Activation('tanh')(s)\n",
    "    s = Dense(2)(s)\n",
    "    s = Activation('softmax')(s)\n",
    "\n",
    "    # sort of residual connection, we only take the last timestamp of the given sensor x[:,index_sensor,-1:]\n",
    "    xin_0 = Activation('relu')(xin)\n",
    "    xin_1 = Lambda(lambda x : x[:,index_sensor,-1:,-1])(xin_0)\n",
    "#     xin_1 = Flatten()(xin_1)\n",
    "    xo_m = Dot(axes=1)([Concatenate()([xo,xin_1]), s])\n",
    "    xo_m = Activation('relu')(xo_m)\n",
    "\n",
    "    model = Model(inputs=[xin], outputs=[xo_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:53.441Z"
    }
   },
   "outputs": [],
   "source": [
    "model = make_model_sensor_conv2d(0, n_sensors=n_sensors,time_slices=len(chosen_times))\n",
    "# model.summary()\n",
    "trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "non_trainable_count = int(\n",
    "    np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "print('Trainable params: {:,}'.format(trainable_count))\n",
    "print('Non-trainable params: {:,}'.format(non_trainable_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:58:54.605Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_test_sensor(id_sensor=4, verbose = 0):\n",
    "    mae_val_list = []\n",
    "    X_tr, y_tr, X_te, y_te = to_array(chosen_times,sensor=longs_np[id_sensor])\n",
    "    # Validation using TS split (just to obtain different MAE estimations, no hyperoptimization for the moment)\n",
    "    for tr_idx, va_idx in TimeSeriesSplit(n_splits=5).split(X_tr):\n",
    "        model = make_model_sensor_conv2d(id_sensor, n_sensors=n_sensors,time_slices=len(chosen_times))\n",
    "        model.compile(opt, loss='mean_absolute_error')\n",
    "        \n",
    "        model.fit(np.atleast_3d(X_tr[tr_idx]), y_tr[tr_idx], batch_size=batch_size, epochs=epochs, validation_data=\n",
    "              (np.atleast_3d(X_tr[va_idx]),y_tr[va_idx]), callbacks=[c2, c3,c4], verbose=verbose)\n",
    "        mae_val_list.append(c3.history['val_loss'][-1])\n",
    "        print('MAE_val ', c3.history['val_loss'][-1])\n",
    "    \n",
    "#     # Testing\n",
    "#     model = make_model_sensor_conv2d(id_sensor, n_sensors=n_sensors,time_slices=len(chosen_times))\n",
    "#     model.compile(opt, loss='mean_absolute_error')\n",
    "#     model.fit(np.atleast_3d(X_tr), y_tr, batch_size=batch_size, epochs=epochs, validation_data=\n",
    "#               (np.atleast_3d(X_te),y_te), callbacks=[c2, c3,c4], verbose=0)\n",
    "    \n",
    "#     print('MAE_test ', c3.history['val_loss'][-1])\n",
    "    return longs_np[id_sensor], np.array(mae_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 AP7\n",
    "\n",
    "MAE_val  75.80979736120578\n",
    "\n",
    "MAE_val  51.144046569770296\n",
    "\n",
    "MAE_val  23.682809347927705\n",
    "\n",
    "MAE_val  48.48229516641444\n",
    "\n",
    "MAE_val  77.36800727015067\n",
    "\n",
    "MAE_test  56.7753012248093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T17:33:12.395597Z",
     "start_time": "2019-04-23T17:07:22.869043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AP7\n",
      "(449885, 9, 17, 1)\n",
      "MAE_val  76.37582733040334\n",
      "MAE_val  58.20910274600245\n",
      "MAE_val  29.512727100253265\n",
      "MAE_val  48.84466819946338\n",
      "MAE_val  77.00408122626646\n",
      "MAE_test  56.87768337969694\n"
     ]
    }
   ],
   "source": [
    "# i=0\n",
    "# print(i, longs_np[i])\n",
    "# sensor, mae = train_and_test_sensor(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T19:59:05.368Z"
    }
   },
   "outputs": [],
   "source": [
    "maes = {}\n",
    "maes_std = {}\n",
    "for i in range(len(longs_np)):\n",
    "    print(i, longs_np[i])\n",
    "    sensor, mae_list = train_and_test_sensor(i)\n",
    "    maes[sensor] = mae_list.mean() \n",
    "    maes_std[sensor] = mae_list.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar",
   "language": "python",
   "name": "solar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
