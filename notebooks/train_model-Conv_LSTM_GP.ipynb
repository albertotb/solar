{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train and test matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that now, X and Y come from different dataframes. Since we want to use the GP image as a feature, but predict the value at each sensor. Maybe it can be used as a suplementary feature and combine both.\n",
    "\n",
    "In order to avoid repeating the GP matrix when creating df_roll, we apply df_shift to df_idx, the dataframe connecting the datetime to the index of the GP array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T22:54:48.566404Z",
     "start_time": "2019-06-17T22:54:48.520221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from utils.build_matrix import df_shift\n",
    "\n",
    "PATH_DATA = '../../data'\n",
    "if not os.path.isdir(PATH_DATA):\n",
    "    print('The folder {} does not exist!'.format(PATH_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T23:37:43.148053Z",
     "start_time": "2019-06-17T23:37:40.846114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 lons: [-158.088, -158.087, -158.086, -158.085, -158.084, -158.083, -158.082, -158.081, -158.08, -158.079, -158.078, -158.077]\n",
      "There are 9 lats: [21.308, 21.309, 21.31, 21.311, 21.312, 21.313, 21.314, 21.315, 21.316]\n",
      "There are 16 sensors: ['AP1', 'AP4', 'AP5', 'AP6', 'AP7', 'DH1', 'DH10', 'DH11', 'DH2', 'DH3', 'DH4', 'DH5', 'DH6', 'DH7', 'DH8', 'DH9']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_gp = pd.read_pickle(os.path.join(PATH_DATA,'oahu_GP.pkl')) \n",
    "#     df.colums = data.colums.drop()\n",
    "except:\n",
    "    print('File {} does not exist.'.format(os.path.join(PATH_DATA,'oahu_GP.pkl')))\n",
    "\n",
    "try:\n",
    "    df_sensors = pd.read_pickle(os.path.join(PATH_DATA,'oahu_min_final.pkl')) \n",
    "#     df.colums = data.colums.drop()\n",
    "except:\n",
    "    print('File {} does not exist.'.format(os.path.join(PATH_DATA,'oahu_min_final.pkl')))\n",
    "\n",
    "assert (df_sensors.index == df_gp.index).all(), 'The index is not the same'\n",
    "df_idx = pd.DataFrame(data = list(range(len(df_gp))),index=df_gp.index,columns=['idx'],dtype='int')\n",
    "\n",
    "# We load the info of the sensors to extract the longitude information\n",
    "try:\n",
    "    info = pd.read_pickle(os.path.join(PATH_DATA,'info.pkl')) \n",
    "#     df.colums = data.colums.drop()\n",
    "except:\n",
    "    print('File {} does not exist.'.format(os.path.join(PATH_DATA,'info.pkl')))\n",
    "\n",
    "# Sorted longitudes\n",
    "lon = info['Longitude'].sort_values(ascending=False).drop('AP3')\n",
    "lat = info['Latitude'].sort_values(ascending=False).drop('AP3')\n",
    "# Sort by lon\n",
    "df_sensors[lon.index]\n",
    "\n",
    "\n",
    "lon_list = df_gp.columns.levels[0].to_numpy()\n",
    "lat_list = df_gp.columns.levels[1].to_numpy()\n",
    "sensors_list = df_sensors.columns.to_numpy()\n",
    "print('There are {} lons: {}'.format(len(lon_list),list(lon_list)))\n",
    "print('There are {} lats: {}'.format(len(lat_list),list(lat_list)))\n",
    "print('There are {} sensors: {}'.format(len(sensors_list),list(sensors_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T23:37:56.043699Z",
     "start_time": "2019-06-17T23:37:54.932819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355387, 12, 9)\n",
      "(355387, 16)\n"
     ]
    }
   ],
   "source": [
    "df_gp.sort_index(axis=1)\n",
    "array_gp = df_gp.to_numpy().reshape(  [-1]+list(map(len,df_gp.columns.levels))   )\n",
    "print(array_gp.shape)\n",
    "\n",
    "array_sensors = df_sensors.to_numpy().reshape(  [-1]+[len(df_sensors.columns)]   )\n",
    "print(array_sensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T23:37:57.374438Z",
     "start_time": "2019-06-17T23:37:57.337421Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_shift(df, periods=1):\n",
    "    return (pd.concat([df] + [ df.tshift(t+1, freq='1min') for t in range(periods) ], axis=1, \n",
    "                      keys=['t'] + [ 't-{:d}'.format(t+1) for t in range(periods) ],\n",
    "                     names = ['time']+df.columns.names)\n",
    ".dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T23:37:58.683038Z",
     "start_time": "2019-06-17T23:37:57.848534Z"
    }
   },
   "outputs": [],
   "source": [
    "df_roll = df_shift(df_idx, periods=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T23:37:58.802172Z",
     "start_time": "2019-06-17T23:37:58.689012Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Split target (time t) and variables (times t-1 to t-width+1)\n",
    "# y = df_roll['t']\n",
    "# X = df_roll.drop(columns='t', level='time')\n",
    "df_train = df_roll[:'2011-07-31']\n",
    "df_test = df_roll['2011-08-01':]\n",
    "\n",
    "y_idx_train = df_train['t'].to_numpy(dtype='int')\n",
    "# Reverse the time index for the X, in case we use Recursive NN\n",
    "X_idx_train = df_train.drop(labels='t',axis=1,level='time').to_numpy(dtype='int')[:,::-1]\n",
    "\n",
    "y_idx_test = df_test['t'].to_numpy(dtype='int')\n",
    "# Reverse the time index for the X\n",
    "X_idx_test = df_test.drop(labels='t',axis=1,level='time').to_numpy(dtype='int')[:,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use as features the GP array and as labels the sensors values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T23:38:01.929151Z",
     "start_time": "2019-06-17T23:38:00.101763Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = array_gp[X_idx_train]\n",
    "y_train = array_sensors[y_idx_train]\n",
    "\n",
    "X_test = array_gp[X_idx_test]\n",
    "y_test = array_sensors[y_idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional predictor\n",
    "\n",
    "First we preprocess the dataset (for the moment, we'll just use as features the t-1 values at each sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to use a 1D convolution, we are going to sort the sensors. For the initial test, we'll just sort them by longitude (from East to West). That way, nearer sensors are in close positions in the tensor, so the 1D convolution may extract useful correlations.\n",
    "\n",
    "Note: many other possible ordenations of the sensors could be added as new channels in the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify which sensor do we want to predict and test.\n",
    "\n",
    "(In the future, we need to discuss how are we going to predict, if just by looping over each sensor, or just give a vectorial prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape, Add, Multiply, Subtract, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, LocallyConnected1D, Conv1D, UpSampling1D, MaxPooling1D, Dot, Concatenate\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture is defined below.\n",
    "\n",
    "Some highlights:\n",
    "* Locally connected works better than pure convolutional at the first layers (probably because the sensors at not located in a uniform grid)\n",
    "* Trick to improve acc: add a final layer combining the convolutional prediction with the persistance prediction, so in case the input is \"strange\", the model could learn to output the persistance prediction (i.e., the previous time-step), which is somewhat reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model_sensor(idx_sensor, n_sensors=16):\n",
    "    ''' Returns a model using all the sensors to predict index_sensor '''\n",
    "    xin = Input(shape=(n_sensors, 1), name='main_input')\n",
    "    x = LocallyConnected1D(8, 7, data_format = 'channels_last', padding='valid')(xin)\n",
    "    x = Activation('relu')(x)\n",
    "    x = LocallyConnected1D(16, 5, data_format = 'channels_last', padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, data_format = 'channels_last', padding='causal')(x)\n",
    "    xl = Flatten()(x)\n",
    "    xl = Dropout(0.2)(xl)\n",
    "    xo = Dense(1)(xl)\n",
    "\n",
    "    # use date info here?\n",
    "    xinf = Flatten()(xin)\n",
    "    s  = Dense(5)(xinf)\n",
    "    s = Activation('tanh')(s)\n",
    "    s = Dense(2)(s)\n",
    "    s = Activation('softmax')(s)\n",
    "\n",
    "    # sort of residual connection\n",
    "    xin_0 = Activation('relu')(xin)\n",
    "    xin_1 = Lambda(lambda x : x[:, idx_sensor, :])(xin_0)\n",
    "    xo_m = Dot(axes=1)([Concatenate()([xo, xin_1]), s])\n",
    "    xo_m = Activation('relu')(xo_m)\n",
    "\n",
    "    model = Model(inputs=[xin], outputs=[xo_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model_sensor_2D(idx_sensor, n_sensors=16):\n",
    "    ''' Returns a model using all the sensors to predict index_sensor '''\n",
    "    xin = Input(shape=(n_sensors, 1), name='lon_input')\n",
    "    x = LocallyConnected1D(8, 7, data_format = 'channels_last', padding='valid')(xin)\n",
    "    x = Activation('relu')(x)\n",
    "    x = LocallyConnected1D(16, 5, data_format = 'channels_last', padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, data_format = 'channels_last', padding='causal')(x)\n",
    "    xl = Flatten()(x) \n",
    "    \n",
    "    yin = Input(shape=(n_sensors, 1), name='lat_input')\n",
    "    y = LocallyConnected1D(8, 7, data_format = 'channels_last', padding='valid')(xin)\n",
    "    y = Activation('relu')(x)\n",
    "    y = LocallyConnected1D(16, 5, data_format = 'channels_last', padding='valid')(x)\n",
    "    y = Activation('relu')(x)\n",
    "    y = Conv1D(32, 3, data_format = 'channels_last', padding='causal')(x)\n",
    "    yl = Flatten()(y)\n",
    "    \n",
    "    xc = Concatenate()([xl, yl])\n",
    "    xc = Dropout(0.2)(xc)\n",
    "    xo = Dense(1)(xc)\n",
    "\n",
    "    # use date info here?\n",
    "    xinf = Flatten()(xin)\n",
    "    s  = Dense(5)(xinf)\n",
    "    s = Activation('tanh')(s)\n",
    "    s = Dense(2)(s)\n",
    "    s = Activation('softmax')(s)\n",
    "\n",
    "    # sort of residual connection\n",
    "    xin_0 = Activation('relu')(xin)\n",
    "    xin_1 = Lambda(lambda x : x[:, idx_sensor, :])(xin_0)\n",
    "    xo_m = Dot(axes=1)([Concatenate()([xo, xin_1]), s])\n",
    "    xo_m = Activation('relu')(xo_m)\n",
    "\n",
    "    model = Model(inputs=[xin, yin], outputs=[xo_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. The below configuration should take 2 minutes in a 16 core CPU\n",
    "(no GPU needed). We are using a huge batch-size to speed up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_array(X_train, y_train, X_test, y_test, id_sensor='AP5', val=0.1):\n",
    "    ''' Converts dataframe to numpy array for predicting any given sensor. val specifies the fraction\n",
    "    of training samples to be used as validation. '''\n",
    "    X_tr1_1_np = X_train.values\n",
    "    y_tr1_1_np = y_train[id_sensor].values\n",
    "    \n",
    "    #val_idx = int((1 - val)*len(y_tr1_1_np))\n",
    "\n",
    "    X_te1_1_np = X_test.values\n",
    "    y_te1_1_np = y_test[id_sensor].values\n",
    "    \n",
    "    #return X_tr1_1_np[:val_idx], y_tr1_1_np[:val_idx], X_tr1_1_np[val_idx:], y_tr1_1_np[val_idx:], X_te1_1_np, y_te1_1_np\n",
    "    return X_tr1_1_np, y_tr1_1_np, X_te1_1_np, y_te1_1_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "lr = 0.0001\n",
    "lr = 0.0001\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "# We add a callback to log metrics and another one to schedule the learning rate\n",
    "\n",
    "#see clr.py in this same folder\n",
    "from utils.clr import CyclicLR\n",
    "\n",
    "c1 = keras.callbacks.BaseLogger(stateful_metrics=None)\n",
    "c2 = CyclicLR(step_size=250, base_lr=lr)\n",
    "c3 = keras.callbacks.History()\n",
    "\n",
    "batch_size = 2048   # as big as possible so we can explore many models\n",
    "epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_sensor(idx_sensor, id_sensor, n_sensors):\n",
    "    X_tr1, y_tr1, X_te1, y_te1 = to_array(X_tr_lon, y_tr_lon, X_te_lon, y_te_lon, id_sensor=id_sensor)\n",
    "    \n",
    "    # Validation using TS split (just to obtain different MAE estimations, no hyperoptimization for the moment)\n",
    "    cv_loss = []\n",
    "    for tr_idx, va_idx in TimeSeriesSplit(n_splits=5).split(X_tr1):\n",
    "        model = make_model_sensor(idx_sensor, n_sensors=n_sensors)\n",
    "        model.compile(opt, loss='mean_absolute_error')\n",
    "        model.fit(np.atleast_3d(X_tr1[tr_idx]), y_tr1[tr_idx], \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  validation_data=(np.atleast_3d(X_tr1[va_idx]), y_tr1[va_idx]), \n",
    "                  callbacks=[c2, c3], \n",
    "                  verbose=0)\n",
    "        cv_loss.append(c3.history['val_loss'][-1])\n",
    "    \n",
    "    # Testing\n",
    "    model = make_model_sensor(idx_sensor, n_sensors=n_sensors)\n",
    "    model.compile(opt, loss='mean_absolute_error')\n",
    "    model.fit(np.atleast_3d(X_tr1), y_tr1, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data=(np.atleast_3d(X_te1), y_te1), \n",
    "              callbacks=[c2, c3], \n",
    "              verbose=0)\n",
    "    test_loss = c3.history['val_loss'][-1]\n",
    "    \n",
    "    print('MAE_val ', cv_loss)\n",
    "    print('MAE_test ', test_loss)\n",
    "    \n",
    "    return test_loss, cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_sensor_2D(idx_sensor, id_sensor, n_sensors):\n",
    "    X_tr1, y_tr1, X_te1, y_te1 = to_array(X_tr_lon, y_tr_lon, X_te_lon, y_te_lon, id_sensor=id_sensor)\n",
    "    X_tr2, y_tr2, X_te2, y_te2 = to_array(X_tr_lat, y_tr_lat, X_te_lat, y_te_lat, id_sensor=id_sensor)\n",
    "    \n",
    "    # Validation using TS split (just to obtain different MAE estimations, no hyperoptimization for the moment)\n",
    "    cv_loss = []\n",
    "    for tr_idx, va_idx in TimeSeriesSplit(n_splits=5).split(X_tr1):\n",
    "        model = make_model_sensor_2D(idx_sensor, n_sensors=n_sensors)\n",
    "        model.compile(opt, loss='mean_absolute_error')\n",
    "        model.fit([np.atleast_3d(X_tr1[tr_idx]), np.atleast_3d(X_tr2[tr_idx])],\n",
    "                  y_tr1[tr_idx], \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  validation_data=([np.atleast_3d(X_tr1[va_idx]), np.atleast_3d(X_tr2[va_idx])], \n",
    "                                   y_tr1[va_idx]), \n",
    "                  callbacks=[c2, c3], \n",
    "                  verbose=0)\n",
    "        cv_loss.append(c3.history['val_loss'][-1])\n",
    "    \n",
    "    # Testing\n",
    "    model = make_model_sensor_2D(idx_sensor, n_sensors=n_sensors)\n",
    "    model.compile(opt, loss='mean_absolute_error')\n",
    "    model.fit([np.atleast_3d(X_tr1), np.atleast_3d(X_tr2)], \n",
    "              y_tr1, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data=([np.atleast_3d(X_te1), np.atleast_3d(X_te2)], \n",
    "                                y_te1), \n",
    "              callbacks=[c2, c3], \n",
    "              verbose=0)\n",
    "    test_loss = c3.history['val_loss'][-1]\n",
    "    \n",
    "    print('MAE_val ', cv_loss)\n",
    "    print('MAE_test ', test_loss)\n",
    "    \n",
    "    return test_loss, cv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AP7\n",
      "MAE_val  [0.1303799112687743, 0.08445334024805945, 0.03708531506906786, 0.07641364477457299, 0.1326083803546976]\n",
      "MAE_test  0.09443650227616966\n",
      "MAE_val  [0.131003138528665, 0.08435243353192248, 0.036877147991262245, 0.07672260898763986, 0.13359125043987616]\n",
      "MAE_test  0.09414855431306496\n"
     ]
    }
   ],
   "source": [
    "maes1 = {}\n",
    "maes2 = {}\n",
    "for idx_sensor, id_sensor in enumerate(lon.index.values):\n",
    "    print(idx_sensor, id_sensor)\n",
    "    maes1[id_sensor], _ = train_and_test_sensor(idx_sensor, id_sensor, n_sensors=16)\n",
    "    maes2[id_sensor], _ = train_and_test_sensor_2D(idx_sensor, id_sensor, n_sensors=16)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AP7    0.094437\n",
       "Name: MAE, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes1 = pd.Series(maes1, name='MAE').sort_values()\n",
    "maes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AP7': 0.09414855431306496}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes2 = pd.Series(maes2, name='MAE').sort_values()\n",
    "maes2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
